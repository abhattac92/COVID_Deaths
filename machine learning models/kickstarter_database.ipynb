{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kickstarter database.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMPIRwUM5Azg",
        "outputId": "c47106cd-39e5-46a5-fb86-7b6780007d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 2.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-2.4.6'\n",
        "spark_version = 'spark-2.4.7'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,673 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,338 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,100 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,110 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [856 kB]\n",
            "Fetched 8,349 kB in 5s (1,585 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2K2G3W05B4J",
        "outputId": "78c2c8bb-4fa6-4994-e0f6-d980e1833b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-03 19:47:39--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  1.02MB/s    in 0.9s    \n",
            "\n",
            "2020-10-03 19:47:41 (1.02 MB/s) - ‘postgresql-42.2.9.jar’ saved [914037/914037]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtt0CYG153sD"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls1-SHjp569g",
        "outputId": "aa1d94e1-4415-41d5-da6e-2f20e1d4c0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://laurentvh-kickstarter.s3.amazonaws.com/ks-projects-201801.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "kick_df = spark.read.csv(SparkFiles.get(\"ks-projects-201801.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "kick_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------+-------------+--------+----------+---------+-------------------+--------+----------+-------+-------+-----------+----------------+-------------+\n",
            "|        ID|                name|      category|main_category|currency|  deadline|     goal|           launched| pledged|     state|backers|country|usd pledged|usd_pledged_real|usd_goal_real|\n",
            "+----------+--------------------+--------------+-------------+--------+----------+---------+-------------------+--------+----------+-------+-------+-----------+----------------+-------------+\n",
            "|1000002330|The Songs of Adel...|        Poetry|   Publishing|     GBP|2015-10-09|  1000.00|2015-08-11 12:12:28|    0.00|    failed|      0|     GB|       0.00|            0.00|      1533.95|\n",
            "|1000003930|Greeting From Ear...|Narrative Film| Film & Video|     USD|2017-11-01| 30000.00|2017-09-02 04:43:57| 2421.00|    failed|     15|     US|     100.00|         2421.00|     30000.00|\n",
            "|1000004038|      Where is Hank?|Narrative Film| Film & Video|     USD|2013-02-26| 45000.00|2013-01-12 00:20:50|  220.00|    failed|      3|     US|     220.00|          220.00|     45000.00|\n",
            "|1000007540|ToshiCapital Reko...|         Music|        Music|     USD|2012-04-16|  5000.00|2012-03-17 03:24:11|    1.00|    failed|      1|     US|       1.00|            1.00|      5000.00|\n",
            "|1000011046|Community Film Pr...|  Film & Video| Film & Video|     USD|2015-08-29| 19500.00|2015-07-04 08:35:03| 1283.00|  canceled|     14|     US|    1283.00|         1283.00|     19500.00|\n",
            "|1000014025|Monarch Espresso Bar|   Restaurants|         Food|     USD|2016-04-01| 50000.00|2016-02-26 13:38:27|52375.00|successful|    224|     US|   52375.00|        52375.00|     50000.00|\n",
            "|1000023410|Support Solar Roa...|          Food|         Food|     USD|2014-12-21|  1000.00|2014-12-01 18:30:44| 1205.00|successful|     16|     US|    1205.00|         1205.00|      1000.00|\n",
            "|1000030581|Chaser Strips. Ou...|        Drinks|         Food|     USD|2016-03-17| 25000.00|2016-02-01 20:05:12|  453.00|    failed|     40|     US|     453.00|          453.00|     25000.00|\n",
            "|1000034518|SPIN - Premium Re...|Product Design|       Design|     USD|2014-05-29|125000.00|2014-04-24 18:14:43| 8233.00|  canceled|     58|     US|    8233.00|         8233.00|    125000.00|\n",
            "| 100004195|STUDIO IN THE SKY...|   Documentary| Film & Video|     USD|2014-08-10| 65000.00|2014-07-11 21:55:48| 6240.57|  canceled|     43|     US|    6240.57|         6240.57|     65000.00|\n",
            "| 100004721| Of Jesus and Madmen|    Nonfiction|   Publishing|     CAD|2013-10-09|  2500.00|2013-09-09 18:19:37|    0.00|    failed|      0|     CA|       0.00|            0.00|      2406.39|\n",
            "| 100005484|    Lisa Lim New CD!|    Indie Rock|        Music|     USD|2013-04-08| 12500.00|2013-03-09 06:42:58|12700.00|successful|    100|     US|   12700.00|        12700.00|     12500.00|\n",
            "|1000055792|  The Cottage Market|        Crafts|       Crafts|     USD|2014-10-02|  5000.00|2014-09-02 17:11:50|    0.00|    failed|      0|     US|       0.00|            0.00|      5000.00|\n",
            "|1000056157|G-Spot Place for ...|         Games|        Games|     USD|2016-03-25|200000.00|2016-02-09 23:01:12|    0.00|    failed|      0|     US|       0.00|            0.00|    200000.00|\n",
            "|1000057089|Tombstone: Old We...|Tabletop Games|        Games|     GBP|2017-05-03|  5000.00|2017-04-05 19:44:18|94175.00|successful|    761|     GB|   57763.78|       121857.33|      6469.73|\n",
            "|1000064368|      Survival Rings|        Design|       Design|     USD|2015-02-28|  2500.00|2015-01-29 02:10:53|  664.00|    failed|     11|     US|     664.00|          664.00|      2500.00|\n",
            "|1000064918|           The Beard|   Comic Books|       Comics|     USD|2014-11-08|  1500.00|2014-10-09 22:27:52|  395.00|    failed|     16|     US|     395.00|          395.00|      1500.00|\n",
            "|1000068480|Notes From London...|     Art Books|   Publishing|     USD|2015-05-10|  3000.00|2015-04-10 21:20:54|  789.00|    failed|     20|     US|     789.00|          789.00|      3000.00|\n",
            "|1000070642|Mike Corey's Dark...|         Music|        Music|     USD|2012-08-17|   250.00|2012-08-02 14:11:32|  250.00|successful|      7|     US|     250.00|          250.00|       250.00|\n",
            "|1000071625|            Boco Tea|          Food|         Food|     USD|2012-06-02|  5000.00|2012-05-03 17:24:32| 1781.00|    failed|     40|     US|    1781.00|         1781.00|      5000.00|\n",
            "+----------+--------------------+--------------+-------------+--------+----------+---------+-------------------+--------+----------+-------+-------+-----------+----------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h08QxG4qFQZ",
        "outputId": "a339f57a-89ee-4499-ce22-75578ccd9bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "kick_df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ID', 'int'),\n",
              " ('name', 'string'),\n",
              " ('category', 'string'),\n",
              " ('main_category', 'string'),\n",
              " ('currency', 'string'),\n",
              " ('deadline', 'string'),\n",
              " ('goal', 'string'),\n",
              " ('launched', 'string'),\n",
              " ('pledged', 'string'),\n",
              " ('state', 'string'),\n",
              " ('backers', 'string'),\n",
              " ('country', 'string'),\n",
              " ('usd pledged', 'string'),\n",
              " ('usd_pledged_real', 'string'),\n",
              " ('usd_goal_real', 'string')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlH3D2C96qcc"
      },
      "source": [
        "# Configure settings for RDS\n",
        "\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://kickstarter.c90yn2pvfvlh.us-east-2.rds.amazonaws.com:5432/postgres\"\n",
        "config = {\"user\":\"postgres\", \n",
        "          \"password\": \"Laurent123!\", \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8LHoAvz7wsB"
      },
      "source": [
        "# Write DataFrame to mask table in RDS\n",
        "kick_df.write.jdbc(url=jdbc_url, table='kickstarter', mode=mode, properties=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It1bjO7LQECe"
      },
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yue2p6qtYnPu"
      },
      "source": [
        "# Change the PySpark dataframes into Pandas dataframes\n",
        "kick_df = kick_df.select(\"*\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MhrY-RyaXAw"
      },
      "source": [
        "## Preprocessing for machine learning\n",
        "\n",
        "We will inspect the data to see if there's any categorical variable or NA values that we need to drop. \n",
        "<pre>\n",
        "# Generate our categorical variable list\n",
        "df_cat = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bc3Beo5aUei"
      },
      "source": [
        "kick_df_cat = kick_df.dtypes[kick_df.dtypes == \"object\"].index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddEkf-Xnb3b3"
      },
      "source": [
        "### I. Inspect whether we need bucketing of variables in categorical columns\n",
        "\n",
        "<pre>\n",
        "df[df_cat].nunique()\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHAqKqA6biRK",
        "outputId": "12ea7582-f617-47c5-e837-ce48d46e2214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "kick_df[kick_df_cat].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "name                375729\n",
              "category              1441\n",
              "main_category          319\n",
              "currency               101\n",
              "deadline              3207\n",
              "goal                  9204\n",
              "launched            377181\n",
              "pledged              63122\n",
              "state                  934\n",
              "backers               4129\n",
              "country                226\n",
              "usd pledged          95383\n",
              "usd_pledged_real    106051\n",
              "usd_goal_real        50734\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsiMiSeLcoIu"
      },
      "source": [
        "### II. Encode the categorical variables\n",
        "First, we will inspect each column on NA values and whether we need to bucket any of the values together in each categorical column.\n",
        "Bucketing them if needed\n",
        "<pre>\n",
        "# Print out each Category value counts of a categorical column\n",
        "cate_counts = df.ColumnName.value_counts()\n",
        "\n",
        "# Visualize the value counts\n",
        "cate_counts.plot.density()\n",
        "\n",
        "# Determine which values to replace \n",
        "replace = list(cate_counts[cate_counts < #].index)\n",
        "\n",
        "# Replace in DataFrame\n",
        "for value in replace:\n",
        "    df.ColumnName = df.ColumnName.replace(value, \"Bucket\")\n",
        "# Check to make sure binning was successful\n",
        "df.ColumnName.value_counts()\n",
        "</pre>\n",
        "\n",
        "Then, move onto encoding the categorical columns. OR just skip to this part if bucketing is unncessary. \n",
        "<pre>\n",
        "# Create a OneHotEncoder instance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the OneHotEncoder using the categorical variable list\n",
        "encode_df = pd.DataFrame(enc.fit_transform(df_ColumnName.values.resape(-1,1)))\n",
        "\n",
        "# Add the encoded variable names to the DataFrame\n",
        "encode_df.columns = enc.get_feature_names(['ColumnName'])\n",
        "encode_df.head()\n",
        "\n",
        "</pre>\n",
        "\n",
        "Finally, merge the encoded DataFrame with the original df and drop the original columns.\n",
        "\n",
        "<pre>\n",
        "df.merge(encode_df, left_index=True, right_index=True).drop(\"ColumnName\", 1)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INj4fXJkcZLs"
      },
      "source": [
        "### III. Decide on columns to drop\n",
        "Inspect null values in each column and decide whether it's worth to drop or fill NA with 0s if needed. \n",
        "Then, we will drop columns that are not adding valuable information such as \"name\" and id\" columns in the kick_df. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TPx-xXgOSs"
      },
      "source": [
        "### IV. Trial and Error for Machine Learning models: Random Forests versus Neural Networks\n",
        "Since we will be developing a model that can identify whether a project will success with the funding, we will be using a binary classification. \n",
        "\n",
        "To get started we will define features and the output.\n",
        "<pre>\n",
        "# Split our preprocessed data into our features and target arrays\n",
        "y = new_df[\"OutputColumn\"].values\n",
        "X = new_df.drop([\"OutputColumn\"],1).values\n",
        "\n",
        "For our dataset, the output column will be \"state\" column.\n",
        "# Split the data into testing and training dataset before standardizing the data.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K083KekOjgvz"
      },
      "source": [
        "### VI. Standardize the data\n",
        "\n",
        "<pre>\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwES3ts1w_7I"
      },
      "source": [
        "### VII. Random Forests\n",
        "<pre>\n",
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GjlFE1hFIXv"
      },
      "source": [
        "### VIII. Support Vector Machine \n",
        "<pre>\n",
        "# Create the SVM model\n",
        "svm = SVC(kernel='sigmoid')\n",
        "# Train the model\n",
        "svm.fit(X_train, y_train)\n",
        "# Evaluate the model\n",
        "y_pred= svm.predict(X_test_scaled)\n",
        "print(f\" SVM model accuracy: {accuracy_score(y_test,y_pred):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XifLyqqFxEpg"
      },
      "source": [
        "### IX. Neural Networks\n",
        "<pre>\n",
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train[0])\n",
        "hidden_nodes_layer1 =  8 # number of neurons will change depending on the nature of the dataset (2-3 times the number of inputs)\n",
        "hidden_nodes_layer2 = 5\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()\n",
        "\n",
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train,y_train,epochs=100)\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha-GYr9ZEpG8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}