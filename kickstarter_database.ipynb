{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kickstarter database.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMPIRwUM5Azg",
        "outputId": "45960a38-6e07-47a0-c8c0-ec612c460631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 2.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-2.4.6'\n",
        "spark_version = 'spark-2.4.7'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [334 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,675 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,693 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,104 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,341 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [857 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,110 kB]\n",
            "Fetched 10.4 MB in 3s (3,695 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2K2G3W05B4J",
        "outputId": "bcf27854-fb4d-48f0-c879-4e83a8dcbb97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 16:13:09--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  5.03MB/s    in 0.2s    \n",
            "\n",
            "2020-10-09 16:13:09 (5.03 MB/s) - ‘postgresql-42.2.9.jar’ saved [914037/914037]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtt0CYG153sD"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls1-SHjp569g",
        "outputId": "ac7716fe-c719-423f-9ffe-a3c7c3158304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://laurentvh-kickstarter.s3.us-east-2.amazonaws.com/latest_data.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "kick_df = spark.read.csv(SparkFiles.get(\"latest_data.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "kick_df.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+------------+------------------------+-------------------+-------------------+-------+-------------------+-------+----------+----------+------------------+------------+------------+------------------+------------------+----------+------------+------------+--------------+-----------+-------------+\n",
            "|Unnamed: 0|backers_count|    category|country_displayable_name|         created_at|           deadline|   goal|        launched_at|pledged|staff_pick|     state|       usd_pledged|sub_category|blurb_length|launch_to_deadline|creation_to_launch|launch_day|deadline_day|launch_month|deadline_month|launch_time|deadline_time|\n",
            "+----------+-------------+------------+------------------------+-------------------+-------------------+-------+-------------------+-------+----------+----------+------------------+------------+------------+------------------+------------------+----------+------------+------------+--------------+-----------+-------------+\n",
            "|         0|            3|        food|       the United States|2014-08-30 01:10:21|2014-10-02 15:44:07|  400.0|2014-09-02 15:44:07|   15.0|     false|    failed|              15.0| small batch|        28.0|                30|                 3|   Tuesday|    Thursday|   September|       October|    2pm-4pm|      2pm-4pm|\n",
            "|         1|            6|  technology|      the United Kingdom|2015-02-18 17:15:56|2015-03-20 20:59:26|  200.0|2015-02-18 21:59:26|   28.0|     false|    failed|       43.04176128|    software|        22.0|                29|                 0| Wednesday|      Friday|    February|         March|   8pm-10pm|     8pm-10pm|\n",
            "|         2|           90| photography|       the United States|2018-07-28 02:07:21|2018-08-27 13:18:38| 2500.0|2018-07-28 13:18:38| 4350.0|     false|successful|            4350.0|  photobooks|        18.0|                30|                 0|  Saturday|      Monday|        July|        August|   12pm-2pm|     12pm-2pm|\n",
            "|         3|          205| photography|      the United Kingdom|2018-02-10 14:42:07|2018-04-05 06:42:32| 9500.0|2018-03-06 07:42:32|10181.0|     false|successful|    14043.22353781|  photobooks|        19.0|                29|                23|   Tuesday|    Thursday|       March|         April|    6am-8am|      6am-8am|\n",
            "|         4|           35|         art|       the United States|2012-07-25 23:21:04|2012-09-11 03:59:00| 4000.0|2012-08-27 20:40:13| 4254.0|     false|successful|            4254.0|  public art|        25.0|                14|                32|    Monday|     Tuesday|      August|     September|   8pm-10pm|      2am-4am|\n",
            "|         6|           38|         art|      the United Kingdom|2019-04-14 13:51:17|2019-05-25 16:10:17|  600.0|2019-04-25 16:10:17| 1536.0|     false|successful|     1987.09937664|    ceramics|        25.0|                30|                11|  Thursday|    Saturday|       April|           May|    4pm-6pm|      4pm-6pm|\n",
            "|         7|           15|  technology|       the United States|2016-07-18 06:08:13|2016-08-21 17:53:41|50000.0|2016-07-22 17:53:41| 2423.5|     false|    failed|            2423.5|    software|        18.0|                30|                 4|    Friday|      Sunday|        July|        August|    4pm-6pm|      4pm-6pm|\n",
            "|         8|           69| photography|                  France|2017-06-13 20:27:22|2017-07-19 17:11:18| 2250.0|2017-06-19 17:11:18| 2720.0|     false|successful|3045.5715968000004|  photobooks|        26.0|                30|                 5|    Monday|   Wednesday|        June|          July|    4pm-6pm|      4pm-6pm|\n",
            "|         9|            3| photography|       the United States|2015-05-01 13:37:20|2015-05-31 17:59:21| 2800.0|2015-05-01 17:59:21|  102.0|     false|    failed|             102.0|      nature|        28.0|                30|                 0|    Friday|      Sunday|         May|           May|    4pm-6pm|      4pm-6pm|\n",
            "|        11|            2|     fashion|                  Canada|2017-10-22 22:37:29|2017-12-07 17:09:47| 6000.0|2017-10-23 16:09:47|  245.0|     false|    failed|       194.0400392|    footwear|        19.0|                45|                 0|    Monday|    Thursday|     October|      December|    4pm-6pm|      4pm-6pm|\n",
            "|        12|          305|film & video|      the United Kingdom|2016-02-08 15:55:58|2016-06-08 08:56:54|75000.0|2016-05-09 08:56:54|77616.0|      true|successful|   111996.92927808|      comedy|        18.0|                30|                90|    Monday|   Wednesday|         May|          June|   8am-10am|     8am-10am|\n",
            "|        13|          192|       music|       the United States|2019-01-12 23:53:52|2019-02-27 15:54:56|15000.0|2019-01-28 15:54:56|15360.0|     false|successful|           15360.0|        none|        22.0|                30|                15|    Monday|   Wednesday|     January|      February|    2pm-4pm|      2pm-4pm|\n",
            "|        14|            2|        food|       the United States|2018-05-12 17:14:10|2018-06-17 21:17:50|25000.0|2018-05-18 21:17:50|   31.0|     false|    failed|              31.0|   cookbooks|        20.0|                30|                 6|    Friday|      Sunday|         May|          June|   8pm-10pm|     8pm-10pm|\n",
            "|        15|           69|  publishing|       the United States|2019-11-27 03:33:10|2020-01-01 15:47:53| 1600.0|2019-12-02 15:47:53|6770.11|     false|successful|           6770.11|  nonfiction|        18.0|                30|                 5|    Monday|   Wednesday|    December|       January|    2pm-4pm|      2pm-4pm|\n",
            "|        16|            1|        food|                 Germany|2016-10-25 18:41:28|2016-11-25 11:06:01| 7500.0|2016-10-26 10:06:01|    1.0|     false|    failed|         1.0879617|   cookbooks|        24.0|                30|                 0| Wednesday|      Friday|     October|      November|  10am-12pm|    10am-12pm|\n",
            "|        17|           40|  journalism|                  France|2019-03-11 10:05:17|2019-05-15 10:00:00| 2500.0|2019-03-19 09:28:28| 2733.0|     false|successful|     3097.94399454|       audio|        19.0|                57|                 7|   Tuesday|   Wednesday|       March|           May|   8am-10am|    10am-12pm|\n",
            "|        18|           13|        food|       the United States|2015-09-01 04:00:17|2015-10-22 23:49:42|10000.0|2015-09-22 23:49:42| 1100.0|     false|    failed|            1100.0| small batch|        18.0|                30|                21|   Tuesday|    Thursday|   September|       October|  10pm-12am|    10pm-12am|\n",
            "|        19|            2|  publishing|      the United Kingdom|2016-04-16 12:10:23|2016-05-18 20:16:37|  700.0|2016-04-18 20:16:37|    2.0|     false|    failed|         2.8404088|   art books|        18.0|                30|                 2|    Monday|   Wednesday|       April|           May|   8pm-10pm|     8pm-10pm|\n",
            "|        20|           85|  publishing|       the United States|2015-07-10 02:40:16|2015-11-01 05:35:19| 4000.0|2015-09-22 05:35:19| 5815.0|      true|successful|            5815.0|   art books|        19.0|                40|                74|   Tuesday|      Sunday|   September|      November|    4am-6am|      4am-6am|\n",
            "|        21|            2|        food|                 Germany|2019-11-18 14:58:26|2020-01-15 17:00:00| 2500.0|2019-11-24 17:44:07|   21.0|     false|    failed|       23.14472958|   cookbooks|        20.0|                51|                 6|    Sunday|   Wednesday|    November|       January|    4pm-6pm|      4pm-6pm|\n",
            "+----------+-------------+------------+------------------------+-------------------+-------------------+-------+-------------------+-------+----------+----------+------------------+------------+------------+------------------+------------------+----------+------------+------------+--------------+-----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h08QxG4qFQZ",
        "outputId": "9f48616c-df35-48bf-e17e-d7862ece541a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kick_df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_c0', 'int'),\n",
              " ('backers_count', 'int'),\n",
              " ('category', 'string'),\n",
              " ('country_displayable_name', 'string'),\n",
              " ('created_at', 'timestamp'),\n",
              " ('deadline', 'timestamp'),\n",
              " ('goal', 'double'),\n",
              " ('id', 'int'),\n",
              " ('launched_at', 'timestamp'),\n",
              " ('pledged', 'double'),\n",
              " ('staff_pick', 'boolean'),\n",
              " ('state', 'string'),\n",
              " ('usd_pledged', 'double'),\n",
              " ('sub_category', 'string'),\n",
              " ('blurb_length', 'double')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlH3D2C96qcc"
      },
      "source": [
        "# Configure settings for RDS\n",
        "\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://kickstarter.c90yn2pvfvlh.us-east-2.rds.amazonaws.com:5432/postgres\"\n",
        "config = {\"user\":\"postgres\", \n",
        "          \"password\": \"Laurent123!\", \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8LHoAvz7wsB"
      },
      "source": [
        "# Write DataFrame to mask table in RDS\n",
        "kick_df.write.jdbc(url=jdbc_url, table='kickstarter', mode=mode, properties=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It1bjO7LQECe"
      },
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yue2p6qtYnPu"
      },
      "source": [
        "# Change the PySpark dataframes into Pandas dataframes\n",
        "kick_df = kick_df.select(\"*\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MhrY-RyaXAw"
      },
      "source": [
        "## Preprocessing for machine learning\n",
        "\n",
        "We will inspect the data to see if there's any categorical variable or NA values that we need to drop. \n",
        "<pre>\n",
        "# Generate our categorical variable list\n",
        "df_cat = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bc3Beo5aUei"
      },
      "source": [
        "kick_df_cat = kick_df.dtypes[kick_df.dtypes == \"object\"].index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddEkf-Xnb3b3"
      },
      "source": [
        "### I. Inspect whether we need bucketing of variables in categorical columns\n",
        "\n",
        "<pre>\n",
        "df[df_cat].nunique()\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHAqKqA6biRK",
        "outputId": "12ea7582-f617-47c5-e837-ce48d46e2214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "kick_df[kick_df_cat].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "name                375729\n",
              "category              1441\n",
              "main_category          319\n",
              "currency               101\n",
              "deadline              3207\n",
              "goal                  9204\n",
              "launched            377181\n",
              "pledged              63122\n",
              "state                  934\n",
              "backers               4129\n",
              "country                226\n",
              "usd pledged          95383\n",
              "usd_pledged_real    106051\n",
              "usd_goal_real        50734\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsiMiSeLcoIu"
      },
      "source": [
        "### II. Encode the categorical variables\n",
        "First, we will inspect each column on NA values and whether we need to bucket any of the values together in each categorical column.\n",
        "Bucketing them if needed\n",
        "<pre>\n",
        "# Print out each Category value counts of a categorical column\n",
        "cate_counts = df.ColumnName.value_counts()\n",
        "\n",
        "# Visualize the value counts\n",
        "cate_counts.plot.density()\n",
        "\n",
        "# Determine which values to replace \n",
        "replace = list(cate_counts[cate_counts < #].index)\n",
        "\n",
        "# Replace in DataFrame\n",
        "for value in replace:\n",
        "    df.ColumnName = df.ColumnName.replace(value, \"Bucket\")\n",
        "# Check to make sure binning was successful\n",
        "df.ColumnName.value_counts()\n",
        "</pre>\n",
        "\n",
        "Then, move onto encoding the categorical columns. OR just skip to this part if bucketing is unncessary. \n",
        "<pre>\n",
        "# Create a OneHotEncoder instance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the OneHotEncoder using the categorical variable list\n",
        "encode_df = pd.DataFrame(enc.fit_transform(df_ColumnName.values.resape(-1,1)))\n",
        "\n",
        "# Add the encoded variable names to the DataFrame\n",
        "encode_df.columns = enc.get_feature_names(['ColumnName'])\n",
        "encode_df.head()\n",
        "\n",
        "</pre>\n",
        "\n",
        "Finally, merge the encoded DataFrame with the original df and drop the original columns.\n",
        "\n",
        "<pre>\n",
        "df.merge(encode_df, left_index=True, right_index=True).drop(\"ColumnName\", 1)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INj4fXJkcZLs"
      },
      "source": [
        "### III. Decide on columns to drop\n",
        "Inspect null values in each column and decide whether it's worth to drop or fill NA with 0s if needed. \n",
        "Then, we will drop columns that are not adding valuable information such as \"name\" and id\" columns in the kick_df. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TPx-xXgOSs"
      },
      "source": [
        "### IV. Trial and Error for Machine Learning models: Random Forests versus Neural Networks\n",
        "Since we will be developing a model that can identify whether a project will success with the funding, we will be using a binary classification. \n",
        "\n",
        "To get started we will define features and the output.\n",
        "<pre>\n",
        "# Split our preprocessed data into our features and target arrays\n",
        "y = new_df[\"OutputColumn\"].values\n",
        "X = new_df.drop([\"OutputColumn\"],1).values\n",
        "\n",
        "For our dataset, the output column will be \"state\" column.\n",
        "# Split the data into testing and training dataset before standardizing the data.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K083KekOjgvz"
      },
      "source": [
        "### VI. Standardize the data\n",
        "\n",
        "<pre>\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwES3ts1w_7I"
      },
      "source": [
        "### VII. Random Forests\n",
        "<pre>\n",
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GjlFE1hFIXv"
      },
      "source": [
        "### VIII. Support Vector Machine \n",
        "<pre>\n",
        "# Create the SVM model\n",
        "svm = SVC(kernel='sigmoid')\n",
        "# Train the model\n",
        "svm.fit(X_train, y_train)\n",
        "# Evaluate the model\n",
        "y_pred= svm.predict(X_test_scaled)\n",
        "print(f\" SVM model accuracy: {accuracy_score(y_test,y_pred):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XifLyqqFxEpg"
      },
      "source": [
        "### IX. Neural Networks\n",
        "<pre>\n",
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train[0])\n",
        "hidden_nodes_layer1 =  8 # number of neurons will change depending on the nature of the dataset (2-3 times the number of inputs)\n",
        "hidden_nodes_layer2 = 5\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()\n",
        "\n",
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train,y_train,epochs=100)\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha-GYr9ZEpG8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}